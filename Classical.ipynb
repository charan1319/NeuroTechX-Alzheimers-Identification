{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L8RYsZcewh-"
      },
      "outputs": [],
      "source": [
        "!pip install asrpy -q\n",
        "!pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcTkp_7stD7F"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from asrpy import ASR\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import pandas as pd\n",
        "import csv\n",
        "import seaborn as sns\n",
        "from scipy.integrate import simpson\n",
        "from scipy import signal\n",
        "import scipy.stats\n",
        "import mne\n",
        "from mne.preprocessing import ICA\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs2PRBz70_hk"
      },
      "source": [
        "## Data visualization and cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx59vXH60yVD"
      },
      "outputs": [],
      "source": [
        "# name of folder all raw data is in (no '/' at the end)\n",
        "folder_name = f\"/content/drive/Shared drivers/NeurotechX Shared Drive/Alzheimer's Dataset\"\n",
        "\n",
        "\n",
        "#what subject number to look at (integer)\n",
        "subject = 5\n",
        "file_path = f\"/content/drive/Shared drives/NeurotechX Shared Drive/Alzheimer's Dataset/derivatives/sub-{subject:03}/eeg/sub-{subject:03}_task-eyesclosed_eeg.set\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HEg4HKa7IeG"
      },
      "outputs": [],
      "source": [
        "def visualize_patient(file_path):\n",
        "\n",
        "    #Import data and preprocess it\n",
        "    # 1) Filter to 0.5-50 Hz\n",
        "    # 2) Resample to 120 Hz\n",
        "    # 3) Re-reference EEG to average value\n",
        "    raw_data = mne.io.read_raw_eeglab(file_path)\n",
        "    raw_data.filter(l_freq=0.5, h_freq=50.0)\n",
        "    raw_data.resample(sfreq=120)\n",
        "    raw_data.set_eeg_reference(ref_channels='average')\n",
        "\n",
        "    # Apply Artifact Subspace Reconstruction -> For artifact removal\n",
        "    asr = ASR(sfreq=raw_data.info[\"sfreq\"], cutoff=15)\n",
        "    asr.fit(raw_data)\n",
        "    raw_data = asr.transform(raw_data)\n",
        "\n",
        "    #visualize raw EEG recordings\n",
        "    raw_data.plot(duration=4, n_channels=19)\n",
        "\n",
        "    #Separate signals into independent components using ICA and visualize\n",
        "    ica = ICA(n_components=19)\n",
        "    ica.fit(raw_data)\n",
        "    ica.plot_components()\n",
        "    #fig.savefig('components.png')\n",
        "\n",
        "    ica.plot_sources(raw_data)\n",
        "\n",
        "# Visualizing one patient's data\n",
        "print('\\n\\nVisualizing data for patient', subject, '\\n\\n')\n",
        "visualize_patient(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWF09IsikzxN"
      },
      "source": [
        "#### Epoch signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOMxp3s7iKgP"
      },
      "outputs": [],
      "source": [
        "def epoch_signal(signal_data, epoch_length, overlap, sampling_rate):\n",
        "    # Calculate the number of samples per epoch\n",
        "    samples_per_epoch = int(epoch_length * sampling_rate)\n",
        "\n",
        "    # Calculate the step size for the overlap\n",
        "    step_size = int(samples_per_epoch * (1 - overlap))\n",
        "\n",
        "    # Create an array to store the epochs\n",
        "    epochs = []\n",
        "\n",
        "    # Iterate over the signal_data with the given step size\n",
        "    for i in range(0, signal_data.shape[1] - samples_per_epoch + 1, step_size):\n",
        "        # Extract the current epoch\n",
        "        epoch = signal_data[:, i:i+samples_per_epoch]\n",
        "\n",
        "        # Append the epoch to the list of epochs\n",
        "        epochs.append(epoch)\n",
        "\n",
        "    # Convert the list of epochs to a numpy array\n",
        "    epochs = np.array(epochs)\n",
        "\n",
        "    return epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT5dHeVP54qF"
      },
      "source": [
        "Calculate relative band power"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiMwDYHPw_kV"
      },
      "outputs": [],
      "source": [
        "def get_bandpowers(subj_epochs, sf=120, win_len=480):\n",
        "    freqs, psd = signal.welch(subj_epochs, sf, nperseg=win_len, axis=2)\n",
        "\n",
        "    freq_res = freqs[1] - freqs[0]\n",
        "    total_power = simpson(psd, freqs, axis=2)\n",
        "\n",
        "    def get_rel_power(low, high, total_power, freqs, psd):\n",
        "        idx_select = np.logical_and(freqs >= low, freqs <= high)\n",
        "        band_power = simpson(psd[:, :, idx_select], freqs[idx_select], axis=2)\n",
        "        rel_power = band_power / total_power\n",
        "        return rel_power\n",
        "\n",
        "    #delta band 0.5-4 Hz\n",
        "    delta_power = get_rel_power(0.5, 4, total_power, freqs, psd)\n",
        "    #theta band 4-8 Hz\n",
        "    theta_power = get_rel_power(4, 8, total_power, freqs, psd)\n",
        "    #alpha band 8-13 Hz\n",
        "    alpha_power = get_rel_power(8, 13, total_power, freqs, psd)\n",
        "    #beta band 13-25 Hz\n",
        "    beta_power = get_rel_power(13, 25, total_power, freqs, psd)\n",
        "    #gamma band 25-45 Hz\n",
        "    gamma_power = get_rel_power(25, 45, total_power, freqs, psd)\n",
        "\n",
        "    return delta_power, theta_power, alpha_power, beta_power, gamma_power\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9Dv1-tzydrs"
      },
      "source": [
        "### Processing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSDRmAzKDEA2"
      },
      "outputs": [],
      "source": [
        "# Pulls participant group type from participants.tsv file\n",
        "# A: Alzheimer group; C: Healthy (Control) group; F: Frontotemporal Dementia group\n",
        "subj_types = []\n",
        "participants_path = f\"/content/drive/Shared drives/NeurotechX Shared Drive/Alzheimer's Dataset/participants.tsv\"\n",
        "with open(participants_path) as file:\n",
        "  for line in file:\n",
        "    l = line.split('\\t')\n",
        "    subj_types.append(l[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grxu_OgtygvA"
      },
      "outputs": [],
      "source": [
        "def get_subject_features(file_path):\n",
        "    #get raw data and pre-process it\n",
        "    raw_data = mne.io.read_raw_eeglab(file_path, verbose=False)\n",
        "    raw_data.filter(l_freq=0.5, h_freq=50.0, verbose=False)\n",
        "    raw_data.resample(sfreq=120)\n",
        "    raw_data.set_eeg_reference(ref_channels='average')\n",
        "\n",
        "    sf = raw_data.info['sfreq']\n",
        "    win_len = 4 * sf #step size of 0.25\n",
        "\n",
        "    # Apply the ASR\n",
        "    asr = ASR(sfreq=sf, cutoff=15)\n",
        "    asr.fit(raw_data)\n",
        "    raw_data = asr.transform(raw_data)\n",
        "\n",
        "    #Try to remove artifacts with ICA\n",
        "    ica = ICA(n_components=19)\n",
        "    ica.fit(raw_data, verbose=False)\n",
        "\n",
        "    #get ica data\n",
        "    ica_data = ica.apply(raw_data, verbose=False).get_data()\n",
        "    epoch_length = 4  # 4 seconds per epoch\n",
        "    overlap = 0.5  # 50% overlap\n",
        "    sampling_rate = 120  # Signal is sampled at 120 Hz\n",
        "\n",
        "    epochs = epoch_signal(ica_data, epoch_length, overlap, sampling_rate)\n",
        "    patient_epoch = epochs.shape[0]\n",
        "\n",
        "    #get time series features\n",
        "    #get average\n",
        "    averages = np.mean(epochs, axis=2)\n",
        "    #get std\n",
        "    stds = np.std(epochs, axis=2)\n",
        "    #get IQR\n",
        "    iqrs = scipy.stats.iqr(epochs, axis=2, rng=(25,75))\n",
        "\n",
        "\n",
        "    #get frequency features: relative spectral power density for EEG Rhythms\n",
        "    delta_power, theta_power, alpha_power, beta_power, gamma_power = get_bandpowers(epochs, sf)\n",
        "\n",
        "    #put everything together in a dataframe: columns are features, rows are 4 second epochs that they are for\n",
        "    features = ['average', 'std', 'iqr', 'delta', 'theta', 'alpha', 'beta', 'gamma',]\n",
        "    col_names = [f'{x}_{y}' for y in features for x in raw_data.ch_names]\n",
        "    subject_df = pd.DataFrame(np.concatenate([averages, stds, iqrs, delta_power, theta_power, alpha_power, beta_power, gamma_power], axis=1), columns=col_names)\n",
        "    return subject_df, patient_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S9YCBQ18HCu"
      },
      "outputs": [],
      "source": [
        "def extract_all_features(folder_name):\n",
        "    # Iterates through all patients to extract features and organize them into a single dataframe\n",
        "    patient_features_list = []\n",
        "    patient_epoch_length = []\n",
        "    for subject in range(1, 89):\n",
        "        print(\"Starting for patient \", subject)\n",
        "        file_path = folder_name + f\"/sub-{subject:03}/eeg/sub-{subject:03}_task-eyesclosed_eeg.set\"\n",
        "        all_features, patient_epoch = get_subject_features(file_path)\n",
        "        print(\"Completed features extraction for patient \", subject)\n",
        "        print('\\n\\n\\n')\n",
        "        #print(all_features)\n",
        "        patient_features_list.append(all_features)\n",
        "        patient_epoch_length.append(patient_epoch)\n",
        "\n",
        "    all_patient_features = pd.concat(patient_features_list).reset_index(drop=True)\n",
        "\n",
        "    return all_patient_features, patient_epoch_length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kxinfom4BRS1"
      },
      "outputs": [],
      "source": [
        "# Extracting all patient features, preprocessing and generating data\n",
        "\n",
        "all_patient_features, patient_epoch_length = extract_all_features(folder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd4XX0Lo8-q7"
      },
      "outputs": [],
      "source": [
        "# Exports data and classification labels to shared drive\n",
        "# This has been commented out because the data's already been exported!\n",
        "# Uncomment the code below if you would like to export the data.\n",
        "# Be sure to change the filename.\n",
        "\n",
        "\n",
        "# all_patient_features.to_csv('/content/drive/Shared drives/NeurotechX Shared Drive/all_patient_features_w_artifact_removal_raw.csv', index=False)\n",
        "\n",
        "# with open('/content/drive/Shared drives/NeurotechX Shared Drive/all_subj_types_w_artifact_removal_raw.csv', 'w', encoding=\"ISO-8859-1\", newline='') as f:\n",
        "#     writer = csv.writer(f)\n",
        "#     writer.writerows(all_subj_types)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDlrsXDdyL4M"
      },
      "source": [
        "## Prepare Classifier Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t81y9fwnd2Ow"
      },
      "outputs": [],
      "source": [
        "def split_data(all_patient_features, patient_epoch_length, subj_types, test_fraction):\n",
        "\n",
        "    # epochs_by_patient is an array of integers, where the element at the i-th index is the subsection of the\n",
        "    # all_patient_features dataframe consisting of all the epochs corresponding to the patient\n",
        "    epochs_by_patient = []\n",
        "    slice_start = 0\n",
        "    for subject in range(len(patient_epoch_length)):\n",
        "        epochs_by_patient.append(all_patient_features[slice_start : slice_start + patient_epoch_length[subject]].to_numpy())\n",
        "        slice_start += patient_epoch_length[subject]\n",
        "\n",
        "    # Renaming data_values to X and sample_class to y\n",
        "    # X = data_values\n",
        "    X = epochs_by_patient # list of 88 elements, i-th element contains the dataframe subsection of features for all epochs corresponding to the i-th subject\n",
        "    # y = sample_class\n",
        "    y = subj_types # list of 88 elements, i-th element contains classification group of the i-th subject (\"A\", \"F\", \"C\")\n",
        "\n",
        "\n",
        "    # Splitting data to train-test using stratified split (by classification group)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_fraction, stratify=y, random_state=1234)\n",
        "\n",
        "    # Separate epochs from their subject-specific arrays to form combined dataframes\n",
        "    # (no longer need to know which subject each epoch comes from)\n",
        "    flat_X_train = []\n",
        "    flat_X_test = []\n",
        "    flat_y_train = []\n",
        "    flat_y_test = []\n",
        "    for i in range(len(X_train)):\n",
        "        flat_X_train.extend(X_train[i])\n",
        "    for i in range(len(X_test)):\n",
        "        flat_X_test.extend(X_test[i])\n",
        "    for i in range(len(y_train)):\n",
        "        flat_y_train.extend([y_train[i]] * len(X_train[i]))\n",
        "    for i in range(len(y_test)):\n",
        "        flat_y_test.extend([y_test[i]] * len(X_test[i]))\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaled_X_train = scaler.fit_transform(flat_X_train)\n",
        "    scaled_X_test = scaler.transform(flat_X_test)\n",
        "\n",
        "    return scaled_X_train, scaled_X_test, flat_y_train, flat_y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJdZbMoruME9"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEWvtVT2vs39"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def get_pca_data(exp_variance, X_train, X_test, y_train, y_test, plot=False):\n",
        "    pca = PCA(n_components=exp_variance)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    #print(X_train_pca.shape)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "    exp_var_pca = pca.explained_variance_ratio_\n",
        "    if plot:\n",
        "        cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
        "        #\n",
        "        # Create the visualization plot\n",
        "        #\n",
        "        plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
        "        plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
        "        plt.ylabel('Explained variance ratio')\n",
        "        plt.xlabel('Principal component index')\n",
        "        plt.legend(loc='best')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return X_train, X_test\n",
        "\n",
        "# sample usage\n",
        "# X_train_pca, X_test_pca = get_pca_data(0.9, scaled_X_train, scaled_X_test, flat_y_train, flat_y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1QoHgY1BpAg"
      },
      "source": [
        "## PLSDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7km_7t562Fh"
      },
      "outputs": [],
      "source": [
        "from sklearn.cross_decomposition import PLSRegression\n",
        "\n",
        "def get_plsda_data(num_components, X_train, X_test, y_train, y_test, plot=False):\n",
        "    plsda = PLSRegression(n_components=num_components)\n",
        "\n",
        "    # Required for plsda, convert class labels into numerical equivalents\n",
        "    flat_y_train_num = []\n",
        "    for label in y_train:\n",
        "      if label == 'A':\n",
        "        flat_y_train_num.append(0)\n",
        "      elif label == 'F':\n",
        "        flat_y_train_num.append(1)\n",
        "      elif label == 'C':\n",
        "        flat_y_train_num.append(2)\n",
        "\n",
        "\n",
        "    X_train_plsda, y_train_plsda = plsda.fit_transform(X_train, flat_y_train_num)\n",
        "\n",
        "    X_test_plsda = plsda.transform(X_test)\n",
        "\n",
        "    return X_train_plsda, X_test_plsda\n",
        "\n",
        "# sample usage\n",
        "# X_train_plsda, X_test_plsda = get_plsda_data(100, scaled_X_train, scaled_X_test, flat_y_train, flat_y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO6e5zpfxfBQ"
      },
      "source": [
        "##Random Decision Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC3Da4vjKO4t"
      },
      "outputs": [],
      "source": [
        "scaled_X_train, scaled_X_test, flat_y_train, flat_y_test = split_data(all_patient_features, patient_epoch_length, subj_types, 0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_XQL5ZyxeOh"
      },
      "outputs": [],
      "source": [
        "def run_rf(scaled_X_train, scaled_X_test, flat_y_train, flat_y_test, dim_reduce=None, n_components=0):\n",
        "\n",
        "    X_train = scaled_X_train\n",
        "    X_test = scaled_X_test\n",
        "\n",
        "    # Check if dimensional reduction\n",
        "    if dim_reduce == 'pca':\n",
        "        X_train, X_test = get_pca_data(n_components, scaled_X_train, scaled_X_test, flat_y_train, flat_y_test)\n",
        "    elif dim_reduce == 'plsda':\n",
        "        X_train, X_test = get_plsda_data(n_components, scaled_X_train, scaled_X_test, flat_y_train, flat_y_test)\n",
        "\n",
        "\n",
        "    # n_estimators is the number of decision trees\n",
        "    # max_features is the number of features each tree uses in decision-making\n",
        "    clf = RandomForestClassifier(n_estimators=100, max_features='sqrt')\n",
        "    clf.fit(X_train, flat_y_train)\n",
        "\n",
        "    # Applies model on test data\n",
        "    y_pred = clf.predict(X_test) # array of predictions of classifier after training\n",
        "\n",
        "    # using metrics module for accuracy calculation. Compares predicted vs. actual\n",
        "    # class labels in test set to calculate accuracy\n",
        "    accuracy = metrics.accuracy_score(flat_y_test, y_pred)\n",
        "\n",
        "    return y_pred, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d53f3YhqG6k"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPa3Xn2-qOxV"
      },
      "outputs": [],
      "source": [
        "# Trains model\n",
        "def run_svm(scaled_X_train, scaled_X_test, flat_y_train, flat_y_test, dim_reduce=None, n_components=0):\n",
        "\n",
        "    X_train = scaled_X_train\n",
        "    X_test = scaled_X_test\n",
        "\n",
        "    if dim_reduce == 'pca':\n",
        "        X_train, X_test = get_pca_data(n_components, scaled_X_train, scaled_X_test, flat_y_train, flat_y_test)\n",
        "    elif dim_reduce == 'plsda':\n",
        "        X_train, X_test = get_plsda_data(n_components, scaled_X_train, scaled_X_test, flat_y_train, flat_y_test)\n",
        "\n",
        "\n",
        "    model = svm.SVC(C=1,\n",
        "                    kernel='poly', #linear, poly, rbf, sigmoid, precomputed\n",
        "                    )\n",
        "    model.fit(X_train, flat_y_train)\n",
        "\n",
        "    # Applies model on test data\n",
        "    y_pred = model.predict(X_test) # array of predictions of classifier after training\n",
        "\n",
        "    # using metrics module for accuracy calculation. Compares predicted vs. actual\n",
        "    # class labels in test set to calculate accuracy\n",
        "    accuracy = metrics.accuracy_score(flat_y_test, y_pred)\n",
        "\n",
        "    return y_pred, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8X1NbVtCwP1"
      },
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InJT-orYCzLK"
      },
      "outputs": [],
      "source": [
        "def run_knn(scaled_X_train, scaled_X_test, flat_y_train, flat_y_test, dim_reduce=None, n_components=0):\n",
        "\n",
        "    X_train = scaled_X_train\n",
        "    X_test = scaled_X_test\n",
        "\n",
        "    if dim_reduce == 'pca':\n",
        "        X_train, X_test = get_pca_data(n_components, scaled_X_train, scaled_X_test, flat_y_train, flat_y_test)\n",
        "    elif dim_reduce == 'plsda':\n",
        "        X_train, X_test = get_plsda_data(n_components, scaled_X_train, scaled_X_test, flat_y_train, flat_y_test)\n",
        "\n",
        "    neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "    # Trains model\n",
        "    neigh.fit(X_train, flat_y_train)\n",
        "\n",
        "    # Applies model on test data\n",
        "    y_pred = neigh.predict(X_test) # array of predictions of classifier after training\n",
        "\n",
        "    # using metrics module for accuracy calculation. Compares predicted vs. actual\n",
        "    # class labels in test set to calculate accuracy\n",
        "    accuracy = metrics.accuracy_score(flat_y_test, y_pred)\n",
        "\n",
        "    return y_pred, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyU0iXa_-lmU"
      },
      "source": [
        "## Multi-run all classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGG30yY9-pAa"
      },
      "outputs": [],
      "source": [
        "rf_res = []\n",
        "knn_res = []\n",
        "svm_res = []\n",
        "rf_cm = []\n",
        "knn_cm = []\n",
        "svm_cm = []\n",
        "y_test_res = []\n",
        "rf_acc = []\n",
        "knn_acc = []\n",
        "svm_acc = []\n",
        "\n",
        "print('Multi-run all classifiers')\n",
        "\n",
        "for i in range(0, 1):\n",
        "    scaled_X_train, scaled_X_test, flat_y_train, flat_y_test = split_data(all_patient_features, patient_epoch_length, subj_types, 0.3)\n",
        "    y_test_res.append(flat_y_test)\n",
        "\n",
        "    # Training random forest model\n",
        "    y_pred, accuracy = run_rf(scaled_X_train, scaled_X_test, flat_y_train, flat_y_test,)\n",
        "\n",
        "    # Making confusion matrix\n",
        "    cm = confusion_matrix(flat_y_test, y_pred, normalize='true')\n",
        "\n",
        "    rf_res.append(y_pred)\n",
        "    rf_acc.append(accuracy)\n",
        "    rf_cm.append(cm)\n",
        "\n",
        "    print('Random forest: ', accuracy)\n",
        "\n",
        "\n",
        "    # Training KNN model\n",
        "    y_pred, accuracy = run_knn(scaled_X_train, scaled_X_test, flat_y_train, flat_y_test,)\n",
        "\n",
        "    # Making confusion matrix\n",
        "    cm = confusion_matrix(flat_y_test, y_pred, normalize='true')\n",
        "\n",
        "    knn_res.append(y_pred)\n",
        "    knn_acc.append(accuracy)\n",
        "    knn_cm.append(cm)\n",
        "\n",
        "    print('KNN: ', accuracy)\n",
        "\n",
        "\n",
        "    # Training SVM model\n",
        "    y_pred, accuracy = run_svm(scaled_X_train, scaled_X_test, flat_y_train, flat_y_test,)\n",
        "\n",
        "    # Making confusion matrix\n",
        "    cm = confusion_matrix(flat_y_test, y_pred, normalize='true')\n",
        "\n",
        "    svm_res.append(y_pred)\n",
        "    svm_cm.append(cm)\n",
        "    svm_acc.append(accuracy)\n",
        "\n",
        "    print('SVM: ', accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrices"
      ],
      "metadata": {
        "id": "rMRt4zo6ZREn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmIze7Iq-k_Y"
      },
      "outputs": [],
      "source": [
        "# Confusion matrices for random forest, SVM, and KNN\n",
        "\n",
        "print('Generating confusion matrices for random forest, SVM, and KNN')\n",
        "\n",
        "rf_cm_avg = np.mean(rf_cm, axis=0)\n",
        "knn_cm_avg = np.mean(knn_cm, axis=0)\n",
        "svm_cm_avg = np.mean(svm_cm, axis=0)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=rf_cm_avg, display_labels=['A','C','F'])\n",
        "disp.plot()\n",
        "disp.ax_.get_images()[0].set_clim(0, 0.7)\n",
        "plt.title(f'Forest, Accuracy: {np.mean(rf_acc):.3}', fontsize=20, fontweight='bold')\n",
        "plt.ylabel('True label', fontsize=16)\n",
        "plt.xlabel('Predicted label', fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.tight_layout()\n",
        "\n",
        "#plt.savefig('/content/drive/Shared drives/NeurotechX Shared Drive/rf_cm.png')\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=knn_cm_avg, display_labels=['A','C','F'])\n",
        "disp.plot()\n",
        "disp.ax_.get_images()[0].set_clim(0, 0.7)\n",
        "plt.title(f'KNN, Accuracy: {np.mean(knn_acc):.3}', fontsize=20, fontweight='bold')\n",
        "plt.ylabel('True label', fontsize=16)\n",
        "plt.xlabel('Predicted label', fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.tight_layout()\n",
        "\n",
        "#plt.savefig('/content/drive/Shared drives/NeurotechX Shared Drive/knn_cm.png')\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=svm_cm_avg, display_labels=['A','C','F'])\n",
        "disp.plot()\n",
        "disp.ax_.get_images()[0].set_clim(0, 0.7)\n",
        "plt.title(f'SVM, Accuracy: {np.mean(svm_acc):.3}', fontsize=20, fontweight='bold')\n",
        "plt.ylabel('True label', fontsize=16)\n",
        "plt.xlabel('Predicted label', fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.tight_layout()\n",
        "#plt.savefig('/content/drive/Shared drives/NeurotechX Shared Drive/svm_cm.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ECN0_qnE3rG"
      },
      "outputs": [],
      "source": [
        "# Generating accuracy bar graphs for random forest, SVM, and KNN\n",
        "\n",
        "print('Generating accuracy bar graphs for random forest, SVM, and KNN')\n",
        "\n",
        "models = ['Random Forest', 'KNN', 'SVM']\n",
        "x_pos = np.arange(len(models))\n",
        "\n",
        "mean_rf = np.mean(rf_acc)\n",
        "std_rf = np.std(rf_acc)\n",
        "\n",
        "mean_knn = np.mean(knn_acc)\n",
        "std_knn = np.std(knn_acc)\n",
        "\n",
        "mean_svm = np.mean(svm_acc)\n",
        "std_svm = np.std(svm_acc)\n",
        "\n",
        "means = [mean_rf, mean_knn, mean_svm]\n",
        "stds = [std_rf, std_knn, std_svm]\n",
        "\n",
        "plt.bar(x_pos, means, yerr=stds, align='center', capsize=10, alpha=0.8, color=['#752D8A', '#D7D9B1', '#84ACCE'])\n",
        "plt.xticks(x_pos, labels=models)\n",
        "plt.ylim([0, 0.75])\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.tight_layout()\n",
        "#plt.savefig('/content/drive/Shared drives/NeurotechX Shared Drive/barplot.png')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
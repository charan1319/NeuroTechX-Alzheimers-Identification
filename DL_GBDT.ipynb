{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bKMEwCyopLoa"
      },
      "source": [
        "## Imports and label setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xXE-MrsWtyX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from scipy import signal\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import mne\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzq4Ylp4Wz56"
      },
      "outputs": [],
      "source": [
        "def epoch_signal(signal, epoch_length, overlap, sampling_rate):\n",
        "    # Get number of samples per epoch and step size for overlap\n",
        "    samples_per_epoch = int(epoch_length * sampling_rate)\n",
        "    step_size = int(samples_per_epoch * (1 - overlap))\n",
        "\n",
        "    epochs = []\n",
        "    for i in range(0, signal.shape[1] - samples_per_epoch + 1, step_size):\n",
        "        # Extract current epoch and add to running list\n",
        "        epoch = signal[:, i:i+samples_per_epoch]\n",
        "        epochs.append(epoch)\n",
        "\n",
        "    epochs = np.array(epochs)\n",
        "    return epochs\n",
        "\n",
        "def get_signal(file_path):\n",
        "  mne.set_log_level('WARNING')\n",
        "  raw_data = mne.io.read_raw_eeglab(file_path)\n",
        "  #bandpass filter between 0.5 and 49 Hz\n",
        "  raw_data.filter(l_freq=0.5, h_freq=49.0)\n",
        "  #Resample to 120, sufficiently above safe frequency to prevent aliasing\n",
        "  raw_data.resample(sfreq=120)\n",
        "  #set reference\n",
        "  raw_data.set_eeg_reference(ref_channels='average')\n",
        "  raw_data = raw_data.get_data()\n",
        "  return raw_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVa6eGowA2dy",
        "outputId": "5b244b7d-e16c-468d-f01d-7f8ef7184004"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Group'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Makes list of participant group types from participants.tsv file\n",
        "# A: Alzheimer group; C: Healthy (Control) group; F: Frontotemporal Dementia group\n",
        "subj_types = []\n",
        "participants_path = f\"/content/drive/Shared drives/NeurotechX Shared Drive/Alzheimer's Dataset/participants.tsv\"\n",
        "with open(participants_path) as file:\n",
        "  for line in file:\n",
        "    l = line.split('\\t')\n",
        "    subj_types.append(l[3])\n",
        "subj_types.pop(0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "me3E5fYdYB71"
      },
      "source": [
        "## Create and store coefficient data in Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfSLIm2UZb6t"
      },
      "outputs": [],
      "source": [
        "#Generate CWT data for all epochs from one patient -- for use when memory is not limiting factor\n",
        "def generate_cwt_data(eeg_data, fs=120):\n",
        "    #Input: 3D array (epochs, channels, samples), fs of data\n",
        "    #Output: 4D array (epochs, channels, frequencies, time) - formatted to\n",
        "    #feed into dataloader after making labels array\n",
        "\n",
        "    # Get the number of epochs, channels, and samples\n",
        "    epochs, channels, samples = eeg_data.shape\n",
        "\n",
        "    # Initialize an empty list to hold the spectrograms\n",
        "    cwts = []\n",
        "\n",
        "    # Loop over the epochs and channels\n",
        "    for i in tqdm(range(epochs.shape[0])):\n",
        "        epoch_cwt = []\n",
        "        for j in range(channels):\n",
        "            # Compute the spectrogram of the current channel in the current epoch\n",
        "            num_scales = 90\n",
        "            min_scale = 1\n",
        "            max_scale = 128\n",
        "            scales = np.logspace(np.log10(min_scale), np.log10(max_scale), num=num_scales)\n",
        "            wavelet = 'morl'  # Morlet wavelet\n",
        "            # wavelet = 'mexh'  # mexican hat wavelet\n",
        "\n",
        "            # perform CWT\n",
        "            coefficients, frequencies = pywt.cwt(eeg_data[i,j,:], scales, wavelet)\n",
        "            epoch_cwt.append(coefficients)\n",
        "\n",
        "        # Append the list of channel cwts to the list of epoch cwts\n",
        "        cwts.append(epoch_cwt)\n",
        "\n",
        "    # Convert the list of cwts to a 4D numpy array and return it\n",
        "    cwt_array= np.array(cwts)\n",
        "\n",
        "    return np.array(cwt_array)\n",
        "\n",
        "def generate_labels(sgram_array, patient_label):\n",
        "    #create labels vector\n",
        "    labels = []\n",
        "    for i in range(0, len(sgram_array)):\n",
        "      labels.append(patient_label)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDwyq0OOjKr9"
      },
      "outputs": [],
      "source": [
        "import pywt\n",
        "\n",
        "# Compute the cwt of all channels for current epoch\n",
        "def get_cwt_per_epoch(epoch_data):\n",
        "  num_scale = 256\n",
        "  min_scale = 1\n",
        "  max_scale = 256\n",
        "  scales = np.linspace(min_scale,max_scale, num_scale)  #decrease scales for faster computation\n",
        "  wavelet = 'morl'  # Morlet wavelet is likely best for these purposes\n",
        "  coeffs_per_channel = []\n",
        "  freq_per_channel = []\n",
        "  # perform CWT\n",
        "  for channel in range(19):\n",
        "    #calculate cwt coefficients and append to array storing coeffs for all channels\n",
        "    coefficients, frequencies = pywt.cwt(epoch_data[channel,:], scales, wavelet)\n",
        "    coeffs_per_channel.append(coefficients)\n",
        "    freq_per_channel.append(frequencies)\n",
        "  coeffs_per_channel = np.array(coeffs_per_channel)\n",
        "\n",
        "  return coeffs_per_channel.astype('float32')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ5DBwg7qwZm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "#Create and store coefficient data in Drive\n",
        "\n",
        "epoch_length = 5  # 5 seconds per epoch\n",
        "overlap = 0.75  # 75% overlap\n",
        "sampling_rate = 120  # Signal is sampled at 120 Hz\n",
        "\n",
        "#Get data for all subjects\n",
        "for subject in range(1,89):\n",
        "  print(\"Starting subject #\", subject)\n",
        "  #get label for current subject\n",
        "  subject_label = subj_types[subject-1]\n",
        "  #set path to the EEG data file for this subject\n",
        "  file_path = f\"/content/drive/Shared drives/NeurotechX Shared Drive/Alzheimer's Dataset/derivatives/sub-{subject:03}/eeg/sub-{subject:03}_task-eyesclosed_eeg.set\"\n",
        "  #set base directory\n",
        "  base_dir = f\"/content/drive/Shared drives/NeurotechX Shared Drive/CWT Coeffs/{subject_label}/\"\n",
        "\n",
        "  #load in EEG data\n",
        "  data = get_signal(file_path)\n",
        "  #split data into epochs\n",
        "  subject_epochs = epoch_signal(data, epoch_length, overlap, sampling_rate)\n",
        "\n",
        "  #process in batches to preserve memory\n",
        "  batch_size = 100\n",
        "  num_batches = int(np.ceil(subject_epochs.shape[0] / batch_size))\n",
        "\n",
        "  for batch in range(num_batches):\n",
        "      start = batch * batch_size\n",
        "      end = min(start + batch_size, subject_epochs.shape[0])\n",
        "      cwts_for_subject = []\n",
        "\n",
        "      for epoch in tqdm(range(start, end)):\n",
        "          coeffs_per_channel = get_cwt_per_epoch(subject_epochs[epoch,:,:])\n",
        "          cwts_for_subject.append(coeffs_per_channel)\n",
        "\n",
        "      filename = f\"patient_{subject:03}_batch{batch+1}.npy\"\n",
        "      file_path = os.path.join(base_dir, filename)\n",
        "      np.save(file_path, cwts_for_subject)\n",
        "\n",
        "      #manually clear memory\n",
        "      del cwts_for_subject, coeffs_per_channel\n",
        "      gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSMiZfPx3RXO"
      },
      "outputs": [],
      "source": [
        "#Parallelize cwt since it's computationally expensive\n",
        "from multiprocessing import Pool\n",
        "\n",
        "def layout_from_epoch(epoch):\n",
        "  coeffs_per_channel = get_cwt_per_epoch(epoch)\n",
        "  final_array = create_layout(coeffs_per_channel)\n",
        "  return final_array\n",
        "\n",
        "\n",
        "#Create pool\n",
        "with Pool() as pool:\n",
        "    results = pool.map(layout_from_epoch, epochs)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Trc3lTqUZJXm"
      },
      "source": [
        "## Create images from cwt data and store in Drive if necessary for space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mT_E3VJpDGP"
      },
      "outputs": [],
      "source": [
        "#Plot channel cwt data in spatially accurate orientation\n",
        "\n",
        "#Can also try changing layout to 4,3,5,3,4 TO 2,5,5,5,2 to avoid too many\n",
        "#\"edges\" for convolutions\n",
        "\n",
        "def create_layout(coeffs_per_channel):\n",
        "  #input: 3D array of coefficients per channel for one epoch\n",
        "  #output: 2D image of all channels' cwt plots spatially organized\n",
        "  #Correct order of channels before plotting\n",
        "  order = [10,0,1,11,2,16,3,12,4,17,5,13,6,18,7,14,8,9,15]\n",
        "  reordered_coeffs_per_channel = [coeffs_per_channel[i] for i in order]\n",
        "\n",
        "  #define grid structure and array shape\n",
        "  grid_structure = [4, 3, 5, 3, 4]\n",
        "  total_columns = 5  # total number of columns in the grid\n",
        "  array_shape = (256, 600)  # the shape of each array\n",
        "\n",
        "  final_array_rows = []\n",
        "\n",
        "  #calculate total width of grid\n",
        "  total_width = total_columns * array_shape[1]\n",
        "\n",
        "  plots_done = 0\n",
        "\n",
        "  # Loop through each row\n",
        "  for num_plots in grid_structure:\n",
        "      #calculate the width of plots and padding for this row\n",
        "      plots_width = num_plots * array_shape[1]\n",
        "      padding_width = (total_width - plots_width) // 2\n",
        "      #create padding arrays\n",
        "      padding = np.zeros((array_shape[0], padding_width))\n",
        "      #create list to hold the plots for this row\n",
        "      plots = [reordered_coeffs_per_channel[plot_num + plots_done] for plot_num in range(num_plots)]\n",
        "      #concatenate padding and plots to create the row\n",
        "      row = np.concatenate([padding] + plots + [padding], axis=1)\n",
        "\n",
        "      final_array_rows.append(row)\n",
        "      plots_done += num_plots\n",
        "\n",
        "  #concatenate rows to create the final array\n",
        "  final_array = np.concatenate(final_array_rows, axis=0)\n",
        "  final_array = np.abs(final_array)\n",
        "\n",
        "  return final_array\n",
        "\n",
        "def create_layouts_all_epochs(all_coeffs):\n",
        "  #input: 4D array of ceofficients for each epoch\n",
        "  #output: 3D array of images for each epoch\n",
        "  all_images_list = []\n",
        "  for epoch in tqdm(range(all_coeffs.shape[0])):\n",
        "    epoch_final_array = create_layout(all_coeffs[epoch,:,:,:])\n",
        "    all_images_list.append(epoch_final_array)\n",
        "\n",
        "  all_images_array = np.concatenate(all_images_list, axis=0)\n",
        "  return all_images_array.astype(float32)\n",
        "\n",
        "\n",
        "#Make image combining all channels per epoch - useful for checking code but not\n",
        "#used for results\n",
        "def make_image(final_array):\n",
        "  # Normalize the array to the range 0-1\n",
        "  log_array = np.log1p(final_array)\n",
        "  normalized_array = (log_array - log_array.min()) / (log_array.max() - log_array.min())\n",
        "\n",
        "  # TO DO : use log normalization with the max across all samples, not within individauls\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  # Plot the final array\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.imshow(final_array, cmap='jet')\n",
        "  plt.colorbar()\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yycnJZKqwMJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_filepaths(directory):\n",
        "  #gets all filepaths within a directory\n",
        "    files = os.listdir(directory)\n",
        "    npy_files = [file for file in files if file.endswith('.npy')]\n",
        "\n",
        "    subject_filepaths = defaultdict(list)  # use a default dict to automatically handle new keys\n",
        "    for npy_file in npy_files:\n",
        "        file_path = os.path.join(directory, npy_file)\n",
        "        patient_number = npy_file.split('_')[1]  # extract patient number from filename\n",
        "        subject_filepaths[patient_number].append(file_path)\n",
        "\n",
        "    return subject_filepaths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKYsJt0Yq0RY"
      },
      "outputs": [],
      "source": [
        "#Create full images for each subject and upload to Drive folder\n",
        "batch_size = 64\n",
        "subject_label = 'A'\n",
        "directory = f\"/content/drive/Shared drives/NeurotechX Shared Drive/CWT Coeffs/{subject_label}/\"\n",
        "subject_filepaths = get_filepaths(directory)\n",
        "\n",
        "for patient, patient_files in subject_filepaths.items():\n",
        "    patient_data = []\n",
        "    for file_path in patient_files:\n",
        "        file_data = np.load(file_path, allow_pickle=True)  # load one file at a time\n",
        "        patient_data.append(file_data)\n",
        "    patient_data = np.concatenate(patient_data, axis=0)\n",
        "\n",
        "    #calculate the number of batches\n",
        "    num_batches = len(patient_data) // batch_size + (len(patient_data) % batch_size != 0)\n",
        "\n",
        "    for batch in range(num_batches):\n",
        "        #determine the start and end index of the batch\n",
        "        start = batch * batch_size\n",
        "        end = min(start + batch_size, len(patient_data))\n",
        "\n",
        "        #process the batch\n",
        "        batch_data = patient_data[start:end]\n",
        "        batch_result = create_layouts_all_epochs(batch_data)\n",
        "\n",
        "        #save batch result to google drive\n",
        "        result_path = f\"/content/drive/Shared drives/NeurotechX Shared Drive/CWT Coeffs/Combined Images/patient_{patient}_{subject_label}_batch_{batch}.npy\"\n",
        "        np.save(result_path, batch_result)\n",
        "\n",
        "        # clear the memory by deleting the variable\n",
        "        del batch_data\n",
        "        del batch_result\n",
        "        np.load.__defaults__ = (None, True, True, 'ASCII')  # this line helps to clear the cache of np.load\n",
        "\n",
        "    del patient_data  #clear memory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPdKYy4VtzvJ"
      },
      "outputs": [],
      "source": [
        "# #Testing: create full images for one subject and upload to Drive folder\n",
        "# subject_label = 'A'\n",
        "# directory = f\"/content/drive/Shared drives/NeurotechX Shared Drive/CWT Coeffs/{subject_label}/\"\n",
        "# subject_filepaths = get_filepaths(directory)\n",
        "\n",
        "# print(subject_filepaths[\"001\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_G9e1F5t-2B"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "subject_label = 'A'\n",
        "patient = \"001\"\n",
        "patient_data = []\n",
        "\n",
        "for file_path in subject_filepaths[patient]:\n",
        "    print(file_path)\n",
        "    file_data = np.load(file_path, allow_pickle=True)  # load one file at a time\n",
        "    patient_data.append(file_data)\n",
        "patient_data = np.concatenate(patient_data, axis=0)\n",
        "\n",
        "# calculate the number of batches\n",
        "num_batches = len(patient_data) // batch_size + (len(patient_data) % batch_size != 0)\n",
        "\n",
        "for batch in range(num_batches):\n",
        "    # determine the start and end index of the batch\n",
        "    start = batch * batch_size\n",
        "    end = min(start + batch_size, len(patient_data))\n",
        "\n",
        "    #process the batch\n",
        "    batch_data = patient_data[start:end]\n",
        "    batch_result = create_layouts_all_epochs(batch_data)\n",
        "\n",
        "    #save batch result to Google Drive\n",
        "    result_path = f\"/content/drive/Shared drives/NeurotechX Shared Drive/CWT Coeffs/Combined Images/patient_{patient}_{subject_label}_batch_{batch}.npy\"\n",
        "    np.save(result_path, batch_result)\n",
        "\n",
        "    # clear the memory by deleting the variable\n",
        "    del batch_data\n",
        "    del batch_result\n",
        "    np.load.__defaults__ = (None, True, True, 'ASCII')  # this line helps to clear the cache of np.load\n",
        "\n",
        "del patient_data  # clear the memory by deleting the variable\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s4Kxs5_13BYv"
      },
      "source": [
        "## Spectrograms -- don't provide enough resolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOHgrLJ90JYC",
        "outputId": "0863f4f6-5d22-4ed6-daff-219e03b30352"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(104, 19, 129, 15)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#testing spectrogram and label generation\n",
        "test_spectrograms = generate_spectrograms(epochs)\n",
        "patient_label = subj_types[subject-1]\n",
        "test_labels = generate_labels(test_spectrograms, patient_label)\n",
        "test_spectrograms.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ7W1jU8zwJE"
      },
      "outputs": [],
      "source": [
        "#Generate set of spectrograms from preprocessed data without ICA // TO DO: Add array for labels for each epoch\n",
        "def generate_spectrograms(eeg_data, fs=120):\n",
        "    #Input: 3D array (epochs, channels, samples), fs of data\n",
        "    #Output: 4D array (epochs, channels, frequencies, time) - formatted to\n",
        "    #feed into dataloader after making labels array\n",
        "\n",
        "    # get number of epochs, channels, and samples\n",
        "    epochs, channels, samples = eeg_data.shape\n",
        "\n",
        "    spectrograms = []\n",
        "\n",
        "    # Loop over the epochs and channels\n",
        "    for i in range(epochs):\n",
        "        epoch_spectrograms = []\n",
        "        for j in range(channels):\n",
        "            # compute spectrogram of the current channel in the current epoch\n",
        "            f, t, Sgram = signal.spectrogram(eeg_data[i, j, :], fs=fs)\n",
        "            epoch_spectrograms.append(Sgram)\n",
        "\n",
        "        #append the list of channel spectrograms to the list of epoch spectrograms\n",
        "        spectrograms.append(epoch_spectrograms)\n",
        "\n",
        "    #convert the list of spectrograms to a 4D array and return\n",
        "    sgram_array= np.array(spectrograms)\n",
        "    return np.array(sgram_array)\n",
        "\n",
        "def generate_labels(sgram_array, patient_label):\n",
        "    #create labels vector\n",
        "    labels = []\n",
        "    for i in range(0, len(sgram_array)):\n",
        "      labels.append(patient_label)\n",
        "    return labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT03cPyjCfXg"
      },
      "outputs": [],
      "source": [
        "#Create arrays for all subjects\n",
        "all_spectrograms = []\n",
        "all_labels = []\n",
        "\n",
        "epoch_length = 4  # 4 seconds per epoch\n",
        "overlap = 0.75  # 75% overlap\n",
        "sampling_rate = 120  # Signal is sampled at 120 Hz\n",
        "\n",
        "for subject in tqdm(range(1, 89)):\n",
        "  #set file path to EEG data for current subject\n",
        "  file_path = f\"/content/drive/Shared drives/NeurotechX Shared Drive/Alzheimer's Dataset/derivatives/sub-{subject:03}/eeg/sub-{subject:03}_task-eyesclosed_eeg.set\"\n",
        "  #get signal and split into epochs\n",
        "  subject_data = get_signal(file_path)\n",
        "  subject_epochs = epoch_signal(subject_data, epoch_length, overlap, sampling_rate)\n",
        "  #create spectrograms and add to running list\n",
        "  subjects_spectrograms = generate_spectrograms(subject_epochs)\n",
        "  all_spectrograms.append(subjects_spectrograms)\n",
        "  #get label for subject and add to list\n",
        "  stored_label = subj_types[subject-1]\n",
        "  subject_labels = generate_labels(subjects_spectrograms, stored_label)\n",
        "  all_labels.append(subject_labels)\n",
        "\n",
        "all_spectrograms = np.concatenate(all_spectrograms, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "#save data to google drive\n",
        "np.save('/content/drive/Shared drives/NeurotechX Shared Drive/spectrograms/larger_spectrograms.npy', all_spectrograms)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0vrKxv1nvPf-"
      },
      "source": [
        "## Create new train/test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZykYC2e_NFw"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def split_data(epochs_by_patient, subj_types, test_fraction):\n",
        "  # Renaming data_values to X and sample_class to y\n",
        "  # X = data_values\n",
        "  X = epochs_by_patient # list of 88 elements, i-th element contains the dataframe subsection of features for all epochs corresponding to the i-th subject\n",
        "  # y = sample_class\n",
        "  y = subj_types # list of 88 elements, i-th element contains classification group of the i-th subject (\"A\", \"F\", \"C\")\n",
        "\n",
        "\n",
        "  # Splitting data to train-test using stratified split (by classification group)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_fraction, stratify=y)\n",
        "\n",
        "  # Separate epochs from their subject-specific arrays to form combined dataframes\n",
        "  # (no longer need to know which subject each epoch comes from)\n",
        "  flat_X_train = []\n",
        "  flat_X_test = []\n",
        "  flat_y_train = []\n",
        "  flat_y_test = []\n",
        "  for i in range(len(X_train)):\n",
        "    flat_X_train.extend(X_train[i])\n",
        "  for i in range(len(X_test)):\n",
        "    flat_X_test.extend(X_test[i])\n",
        "  for i in range(len(y_train)):\n",
        "    flat_y_train.extend([y_train[i]] * len(X_train[i]))\n",
        "  for i in range(len(y_test)):\n",
        "    flat_y_test.extend([y_test[i]] * len(X_test[i]))\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  scaled_X_train = scaler.fit_transform(flat_X_train)\n",
        "  scaled_X_test = scaler.transform(flat_X_test)\n",
        "\n",
        "  return scaled_X_train, scaled_X_test, flat_y_train, flat_y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxM0WKfG-0kx"
      },
      "outputs": [],
      "source": [
        "# Import epochs_by_patient and subj_types from shared drive\n",
        "import pickle\n",
        "\n",
        "epochs_by_patient = pickle.load(open(\"/content/drive/Shared drives/NeurotechX Shared Drive/epochs_by_patient.p\", \"rb\"))\n",
        "subj_types = pickle.load(open(\"/content/drive/Shared drives/NeurotechX Shared Drive/subj_types.p\", \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvXKOLyWB9ch"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "#Prepare training and testing labels - fitting encoder ensures one-hot encodings\n",
        "#are consistently assigned to same class labels between train and test sets\n",
        "#since encoding is done after splitting\n",
        "def prep_data(train_features, test_features, train_labels, test_labels):\n",
        "\n",
        "  le = LabelEncoder()     #make and fit the label encoder\n",
        "  le.fit(train_labels)\n",
        "\n",
        "  train_classes = le.transform(train_labels)    #transform the training and test labels into integer classes\n",
        "  test_classes = le.transform(test_labels)\n",
        "\n",
        "  ohe = OneHotEncoder(sparse=False)           #fit the one-hot encoder\n",
        "  ohe.fit(train_classes.reshape(-1, 1))\n",
        "\n",
        "  train_one_hot = ohe.transform(train_classes.reshape(-1, 1)) # transform to one-hot encodings\n",
        "  test_one_hot = ohe.transform(test_classes.reshape(-1, 1))\n",
        "\n",
        "  train_labels = torch.tensor(train_one_hot).float()\n",
        "  test_labels = torch.tensor(test_one_hot).float()\n",
        "\n",
        "  # Prepare training features\n",
        "\n",
        "  train_features = torch.tensor(train_features).float()\n",
        "  test_features = torch.tensor(test_features).float()\n",
        "\n",
        "  return train_features, test_features, train_labels, test_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OePliYzvtRb"
      },
      "outputs": [],
      "source": [
        "train_features, test_features, train_labels, test_labels = split_data(epochs_by_patient, subj_types, 0.3)\n",
        "train_features, test_features, train_labels, test_labels = prep_data(train_features, test_features, train_labels, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIGJZOr-RkPw"
      },
      "outputs": [],
      "source": [
        "#Incorporate class weights -- didn't help in practice\n",
        "# If train_labels is a numpy array of one-hot encoded labels\n",
        "train_labels_class_indices = np.array(np.argmax(train_labels, axis=1))\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels_class_indices), y=train_labels_class_indices)\n",
        "\n",
        "# Convert class weights to a PyTorch tensor\n",
        "class_weights = torch.tensor(class_weights).float()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "shtWv6ezBFQu"
      },
      "source": [
        "## Implement MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sqvcvFqEBRo"
      },
      "outputs": [],
      "source": [
        "# Create a TensorDataset and a DataLoader\n",
        "train_dataset = TensorDataset(train_features, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataset = TensorDataset(test_features, test_labels)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(152, 80)\n",
        "        self.fc2 = nn.Linear(80, 20)\n",
        "        self.fc3 = nn.Linear(10, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = (self.fc3(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6_SJIm3WYO0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MLP()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#option to include class weights - doesn't help in practice\n",
        "# criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "\n",
        "    #training loop\n",
        "    for batch_features, batch_labels in train_dataloader: # loop through batches\n",
        "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_features)\n",
        "        loss = criterion(output, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    #track training accuracy every 5 epochs\n",
        "    if epoch % 5 == 0:\n",
        "        model.eval()\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_features, batch_labels in train_dataloader:\n",
        "                batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "                output = model(batch_features)\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                total_train += batch_labels.size(0)\n",
        "                correct_train += (predicted == torch.argmax(batch_labels, dim=1)).sum().item()\n",
        "\n",
        "        train_accuracy = 100 * correct_train / total_train\n",
        "        train_accuracy_list.append(train_accuracy)\n",
        "\n",
        "        #evaluate on test data\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_features, batch_labels in test_dataloader:\n",
        "                batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "                output = model(batch_features)\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                total_test += batch_labels.size(0)\n",
        "                correct_test += (predicted == torch.argmax(batch_labels, dim=1)).sum().item()\n",
        "\n",
        "        test_accuracy = 100 * correct_test / total_test\n",
        "        test_accuracy_list.append(test_accuracy)\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}, Train Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}')\n",
        "\n",
        "#plotting train/test accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(0, len(train_accuracy_list)*5, 5), train_accuracy_list, label='Train')\n",
        "plt.plot(range(0, len(test_accuracy_list)*5, 5), test_accuracy_list, label='Test')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJZvWgSwVP3J"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_features, batch_labels in test_dataloader:\n",
        "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "        output = model(batch_features)\n",
        "\n",
        "        # Apply softmax to output.\n",
        "        pred_probabilities = F.softmax(output, dim=1)\n",
        "\n",
        "        # Take the class with the highest probability from the output as prediction\n",
        "        _, predicted = torch.max(pred_probabilities.data, 1)\n",
        "\n",
        "        predictions.extend(predicted.cpu().numpy().tolist())\n",
        "        true_labels.extend(torch.argmax(batch_labels, dim=1).cpu().numpy().tolist())\n",
        "\n",
        "#Classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(true_labels, predictions))\n",
        "\n",
        "#Confusion matrix\n",
        "conf_mat = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', cbar = False)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "class_names = [\"Alzheimer's\", \"Control\", 'FT Dementia']\n",
        "plt.xticks(ticks=np.arange(len(class_names))+.5, labels=class_names, ha='center')\n",
        "plt.yticks(ticks=np.arange(len(class_names))+.5, labels=class_names)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xTaTxQVuWrBg"
      },
      "source": [
        "## Try grid search for oversimplified hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Qjjs143Gs_O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "        self.layers.append(nn.Linear(hidden_size, output_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = torch.relu(layer(x))\n",
        "        x = self.layers[-1](x)\n",
        "        return x\n",
        "\n",
        "#define hyperparameters for grid search\n",
        "learning_rates = [0.1, 0.01, 0.001]\n",
        "num_layers = [1, 2, 3]\n",
        "hidden_sizes = [32, 64, 128]\n",
        "\n",
        "#to hold best model and parameters\n",
        "best_model = None\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "#grid search over hyperparameters\n",
        "for lr in learning_rates:\n",
        "    print('checking learning rate = ', lr)\n",
        "    for layers in num_layers:\n",
        "        print('checking num layers = ', layers)\n",
        "        for hidden_size in hidden_sizes:\n",
        "            print('checking hidden size = ', hidden_size)\n",
        "            model = MLP(input_size=152, hidden_size=hidden_size, num_layers=layers, output_size=3)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "            #training loop\n",
        "            for epoch in range(40):  #set appropriate number of epochs\n",
        "                for batch_features, batch_labels in train_dataloader:\n",
        "                    optimizer.zero_grad()\n",
        "                    output = model(batch_features)\n",
        "                    loss = criterion(output, torch.argmax(batch_labels, dim=1))\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            #evaluation\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                for batch_features, batch_labels in test_dataloader:\n",
        "                    output = model(batch_features)\n",
        "                    _, predicted = torch.max(output.data, 1)\n",
        "                    total += batch_labels.size(0)\n",
        "                    correct += (predicted == torch.argmax(batch_labels, dim=1)).sum().item()\n",
        "\n",
        "            accuracy = correct / total\n",
        "            print(f'Learning rate: {lr}, Hidden layers: {layers}, Hidden size: {hidden_size}, Accuracy: {accuracy}')\n",
        "\n",
        "            #save model if it has the best accuracy so far\n",
        "            if accuracy > best_accuracy:\n",
        "                best_model = model\n",
        "                best_accuracy = accuracy\n",
        "                best_params = {'Learning rate': lr, 'Hidden layers': layers, 'Hidden size': hidden_size}\n",
        "\n",
        "print(f'Best model parameters: {best_params}, Best model accuracy: {best_accuracy}')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3G5CiqVSVqrS"
      },
      "source": [
        "## Implement Gradient Boosted Decision Trees with XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7VMvJlJOiHn",
        "outputId": "00577b24-2262-4ad9-f46a-41eb01e7c73b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: XGBoost in /usr/local/lib/python3.10/dist-packages (1.7.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from XGBoost) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from XGBoost) (1.10.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akuLgogiVuRh"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_features, test_features, train_labels, test_labels = split_data(epochs_by_patient, subj_types, 0.2)\n",
        "train_features, test_features, train_labels, test_labels = prep_data(train_features, test_features, train_labels, test_labels)\n",
        "\n",
        "# Convert data to DMatrix format for XGBoost\n",
        "train_labels = np.argmax(train_labels, axis=1)\n",
        "test_labels = np.argmax(test_labels, axis=1)\n",
        "dtrain = xgb.DMatrix(train_features, label=train_labels)\n",
        "dtest = xgb.DMatrix(test_features, label=test_labels)\n",
        "\n",
        "#specify parameters\n",
        "param = {\n",
        "    'max_depth': 11,  # maximum tree depth\n",
        "    'eta': 0.3,  #learning rate\n",
        "    'objective': 'multi:softprob',  #loss function\n",
        "    'num_class': 3,\n",
        "    'tree_method': 'gpu_hist'}  #uses gpu\n",
        "\n",
        "#train model\n",
        "num_round = 30 #number of training rounds\n",
        "bst = xgb.train(param, dtrain, num_round)\n",
        "\n",
        "#make prediction\n",
        "preds = bst.predict(dtest)\n",
        "preds_class = np.argmax(preds, axis=1)\n",
        "\n",
        "#calculate accuracy\n",
        "accuracy = accuracy_score(test_labels, preds_class)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxMrVTMyYsrd",
        "outputId": "ee7ab089-f5dd-4ee5-aae7-531f8905ed0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.59      0.53      2509\n",
            "           1       0.65      0.71      0.68      2635\n",
            "           2       0.55      0.30      0.39      1739\n",
            "\n",
            "    accuracy                           0.56      6883\n",
            "   macro avg       0.56      0.53      0.53      6883\n",
            "weighted avg       0.57      0.56      0.55      6883\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_labels, preds_class))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2mxzXLuDqZu7"
      },
      "source": [
        "## Hyperparameter tuning with grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqyfda5_ZgVo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "train_features, test_features, train_labels, test_labels = split_data(epochs_by_patient, subj_types, 0.1)\n",
        "train_features, test_features, train_labels, test_labels = prep_data(train_features, test_features, train_labels, test_labels)\n",
        "\n",
        "#convert data to DMatrix format for XGBoost\n",
        "train_labels = np.argmax(train_labels, axis=1)\n",
        "test_labels = np.argmax(test_labels, axis=1)\n",
        "dtrain = xgb.DMatrix(train_features, label=train_labels)\n",
        "dtest = xgb.DMatrix(test_features, label=test_labels)\n",
        "\n",
        "#create a dictionary of hyperparameters\n",
        "param_grid = {\n",
        "    'max_depth': range(1, 10),\n",
        "    'n_estimators': range(1, 50, 5)  # number of rounds/trees\n",
        "}\n",
        "\n",
        "#create base model\n",
        "xgb_model = xgb.XGBClassifier(objective='multi:softprob', num_class=3, tree_method='gpu_hist')\n",
        "\n",
        "#instantiate grid search model\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=3)\n",
        "\n",
        "\n",
        "#fit the grid search to the data\n",
        "grid_search.fit(train_features, train_labels)\n",
        "\n",
        "#print the best parameters\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "\n",
        "#print the performance of each combination\n",
        "cv_results = grid_search.cv_results_\n",
        "for mean_score, params in zip(cv_results[\"mean_test_score\"], cv_results[\"params\"]):\n",
        "    print(params, \" Mean accuracy: \", mean_score)\n",
        "\n",
        "#use the best model to make predictions\n",
        "best_model = grid_search.best_estimator_\n",
        "preds = best_model.predict(test_features)\n",
        "\n",
        "#calculate accuracy\n",
        "accuracy = accuracy_score(test_labels, preds)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmzmioRfoh10"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_features, batch_labels in test_dataloader:\n",
        "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "        output = model(batch_features)\n",
        "\n",
        "        #get prediction\n",
        "        pred_probabilities = F.softmax(output, dim=1)\n",
        "        _, predicted = torch.max(pred_probabilities.data, 1)\n",
        "\n",
        "        #store predictions\n",
        "        predictions.extend(predicted.cpu().numpy().tolist())\n",
        "        true_labels.extend(torch.argmax(batch_labels, dim=1).cpu().numpy().tolist())\n",
        "\n",
        "#Classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(true_labels, predictions))\n",
        "\n",
        "#Confusion matrix\n",
        "conf_mat = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', cbar = False)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "class_names = [\"Alzheimer's\", \"Control\", 'FT Dementia']\n",
        "plt.xticks(ticks=np.arange(len(class_names))+.5, labels=class_names, ha='center')\n",
        "plt.yticks(ticks=np.arange(len(class_names))+.5, labels=class_names)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5yK1rOXLMFXl"
      },
      "source": [
        "## Implement CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bR7z5Cr0QiD"
      },
      "outputs": [],
      "source": [
        "#Create dataset class\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, data, labels=None, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        if self.labels is not None:\n",
        "            return sample, self.labels[idx]\n",
        "        else:\n",
        "            return sample\n",
        "\n",
        "#Create dataloader for spectrograms\n",
        "\n",
        "def create_dataloader(data, labels=None, transform=None, batch_size=32, shuffle=True):\n",
        "    dataset = EEGDataset(data, labels, transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    return dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mj5oi6fKQk6"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "all_spectrograms = all_spectrograms.astype('float32')\n",
        "labelcoder = LabelEncoder()\n",
        "encoded_labels = labelcoder.fit_transform(all_labels)\n",
        "\n",
        "#To convert back after predictions\n",
        "# y_pred_labels = labelcoder.inverse_transform(y_pred)\n",
        "\n",
        "dataloader = create_dataloader(all_spectrograms, encoded_labels, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVABMxNjMS4q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(19, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hutak_aVPEw0"
      },
      "outputs": [],
      "source": [
        "#Check for device\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tFO7xR9Me5W"
      },
      "outputs": [],
      "source": [
        "num_epochs = 15\n",
        "batch_size = 128\n",
        "learning_rate = 1e-5\n",
        "\n",
        "model = AlexNet()\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model.train()\n",
        "loss_tracker = torch.zeros(num_epochs)\n",
        "\n",
        "for i in tqdm(range(num_epochs)):\n",
        "  for (data, label) in dataloader:\n",
        "    label = torch.flatten(label)\n",
        "    data, label = data.to(device), label.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model.forward(data)\n",
        "    loss = criterion(outputs, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  loss_tracker[i] = loss.item()\n",
        "  print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe-JACIzThW6"
      },
      "outputs": [],
      "source": [
        "size_dataset = len(dataloader.dataset)\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    #Iterate through all datapoints in dataloader\n",
        "    for (data, label) in dataloader:\n",
        "      label = torch.flatten(label)\n",
        "      data, label = data.to(device), label.to(device)\n",
        "      outputs = model(data)\n",
        "      correct += (outputs.argmax(dim = 1)==label).sum().item()\n",
        "\n",
        "accuracy = correct / size_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAYADCZENsky"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(19, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53R7zbqMLJnK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from itertools import cycle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load all files\n",
        "all_files = os.listdir('/path/to/your/directory')\n",
        "# Get unique subjects\n",
        "subjects = set(file.split('_')[1] for file in all_files)\n",
        "\n",
        "# Split subjects into train and test\n",
        "train_subjects, test_subjects = train_test_split(list(subjects), test_size=0.2, random_state=42)\n",
        "\n",
        "# Group files by subject and label\n",
        "train_files = {label: [] for label in ('class1', 'class2', 'class3')}\n",
        "test_files = {label: [] for label in ('class1', 'class2', 'class3')}\n",
        "for file in all_files:\n",
        "    subject = file.split('_')[1]\n",
        "    label = file.split('_')[2]  # assuming the label is the third element when splitting by '_'\n",
        "    if subject in train_subjects:\n",
        "        train_files[label].append(file)\n",
        "    else:\n",
        "        test_files[label].append(file)\n",
        "\n",
        "# Calculate class proportions\n",
        "total_train_files = sum(len(files) for files in train_files.values())\n",
        "train_class_proportions = {label: len(files) / total_train_files for label, files in train_files.items()}\n",
        "\n",
        "def shuffle_in_unison(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    combined = np.c_[a, b]  # Stack the two lists column-wise\n",
        "    np.random.shuffle(combined)  # Shuffle the combined array\n",
        "    return combined[:, 0], combined[:, 1]  # Split the shuffled array into two\n",
        "\n",
        "def load_data(file):\n",
        "    # Extract label from filename\n",
        "    label = file.split('_')[2]  # assuming the label is the third element when splitting by '_'\n",
        "\n",
        "    # Load data from file\n",
        "    data = np.load(file)  # You need to adjust this line based on where your files are stored\n",
        "\n",
        "    # Create a label array of the same length as the data\n",
        "    labels = np.full(len(data), label)\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "def data_generator(label_groups, class_proportions, batch_size):\n",
        "    # Create an iterator for each group\n",
        "    iterators = {label: iter(cycle(files)) for label, files in label_groups.items()}\n",
        "\n",
        "    while True:\n",
        "        batch_data = []\n",
        "        batch_labels = []\n",
        "        # For each class, sample proportional to its representation\n",
        "        for label, proportion in class_proportions.items():\n",
        "            num_samples = round(proportion * batch_size)\n",
        "            file = next(iterators[label])\n",
        "            # Load the data and labels from the file\n",
        "            data, labels = load_data(file)\n",
        "\n",
        "            # Add a proportional amount of data and labels to the batch\n",
        "            batch_data.extend(data[:num_samples])\n",
        "            batch_labels.extend(labels[:num_samples])\n",
        "\n",
        "        # Shuffle the data and labels in the same order to prevent ordering bias\n",
        "        shuffle_in_unison(batch_data, batch_labels)\n",
        "\n",
        "        # Yield the batch\n",
        "        yield np.array(batch_data), np.array(batch_labels)\n",
        "\n",
        "# Use the data generator to train the model\n",
        "model.fit(data_generator(train_files, train_class_proportions, batch_size=64), steps_per_epoch=100, epochs=10)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
